{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Multi-class Classification and Neural Network\n",
    "> **FULL MARKS = 10**\n",
    "\n",
    "In this assignment, you are going to implement your own neural network to do multi-class classification. We use one-vs-all strategy here by training multiple binary classifiers (one for each class).\n",
    "\n",
    "Please notice that you can only use numpy and scipy.optimize.minimize. **No** library versions of other method are allowed.  . Follow the instructions, you will need to fill the blanks to make it functional. The process is similar to the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    idx = np.arange(len(X))\n",
    "    train_size = int(len(X) * 2/3)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(num_in, num_out):\n",
    "    '''\n",
    "    :param num_in: the number of input units in the weight array\n",
    "    :param num_out: the number of output units in the weight array\n",
    "    '''\n",
    "\n",
    "    # Note that 'W' contains both weights and bias, you can consider W[0, :] as bias\n",
    "    W = np.zeros((1 + num_in, num_out))\n",
    "    \n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement Xavier/Glorot uniform initialization                                  #\n",
    "    #                                                                                 #\n",
    "    # Hint: you can find the reference here:                                          #\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal   #\n",
    "    ###################################################################################\n",
    "    \n",
    "    x_uniform = np.sqrt(6.0 / (num_in + num_out))\n",
    "    W = np.random.uniform(-x_uniform,x_uniform,W.shape)\n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement sigmoid function:                                                     #\n",
    "    #                             sigmoid(x) = 1/(1+e^(-x))                           #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement tanh function:                                                        #\n",
    "    #                     tanh(x) = (e^x-e^(-x)) / (e^x+e^(-x))                       #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = np.tanh(x)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the sigmoid function evaluated at x.                   #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    grad = sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the tanh function evaluated at x.                      #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    grad = (1 - tanh(x)**2)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, X):\n",
    "    '''\n",
    "    :param W: weights (including biases) of the neural network. It is a list containing both W_hidden and W_output.\n",
    "    :param X: input. Already added one additional column of all \"1\"s.\n",
    "    '''\n",
    "\n",
    "    # Shape of W_hidden: [num_feature+1, num_hidden]\n",
    "    # Shape pf W_output: [num_hidden+1, num_output]\n",
    "    W_hidden, W_output = W\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement the forward pass. You need to compute four values:                    #\n",
    "    # z_hidden, a_hidden, z_output, a_output                                          #\n",
    "    #                                                                                 #\n",
    "    # Note that our neural network consists of three layers:                          #\n",
    "    # Input -> Hidden -> Output                                                       #\n",
    "    # The activation function in hidden layer is 'tanh'                               #\n",
    "    # The activation function in output layer is 'sigmoid'                            #\n",
    "    ###################################################################################\n",
    "    \n",
    "    W_hidden = np.array(W_hidden)\n",
    "    W_output = np.array(W_output)\n",
    "    \n",
    "    z_hidden = np.dot(X,W_hidden)\n",
    "    a_hidden = tanh(z_hidden)\n",
    "    a_hidden = np.concatenate([np.ones(( X.shape[0], 1)), a_hidden], axis=1)\n",
    "    z_output = np.dot(a_hidden,W_output)\n",
    "    a_output = sigmoid(z_output)\n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # z_hidden is the raw output of hidden layer, a_hidden is the result after performing activation on z_hidden\n",
    "    # z_output is the raw output of output layer, a_output is the result after performing activation on z_output\n",
    "    return {'z_hidden': z_hidden, 'a_hidden': a_hidden,\n",
    "            'z_output': z_output, 'a_output': a_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_funtion(W, X, y, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param W: a 1D array containing all weights and biases.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "    \n",
    "    m = y.size\n",
    "\n",
    "    # Reshape W back into the parameters W_hidden and W_output\n",
    "    W_hidden = np.reshape(W[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "\n",
    "    W_output = np.reshape(W[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    # Initialize grads\n",
    "    W_hidden_grad = np.zeros(W_hidden.shape)\n",
    "    W_output_grad = np.zeros(W_output.shape)\n",
    "\n",
    "    # Add one column of \"1\"s to X\n",
    "    X_input = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    ##########################################################################################\n",
    "    # Full Mark: 3                                                                           #\n",
    "    # TODO:                                                                                  #\n",
    "    # 1. Transform y to one-hot encoding. Implement binary cross-entropy loss function       #\n",
    "    # (Hint: Use the forward function to get necessary outputs from the model)               #\n",
    "    #                                                                                        #\n",
    "    # 2. Add L2 regularization to all weights in loss                                        #\n",
    "    # (Note that we will not add regularization to bias)                                     #\n",
    "    #                                                                                        #\n",
    "    # 3. Compute the gradient for W_hidden and W_output (including both weights and biases)  #\n",
    "    # (Hint: use chain rule, and the sigmoid gradient, tanh gradient you have                #\n",
    "    # implemented above. Don't forget to add the gradient of regularization term)            #\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # One hot encoding\n",
    "    shape = (y.size, y.max()+1)\n",
    "    one_hot = np.zeros(shape)\n",
    "    rows = np.arange(y.size)\n",
    "    one_hot[rows, y] = 1\n",
    "    \n",
    "    # forward pass\n",
    "    f_pass = forward((W_hidden,W_output),X_input)\n",
    "    a_hidden = f_pass['a_hidden']\n",
    "    a_output = f_pass['a_output']\n",
    "    \n",
    "    # cross-entropy loss\n",
    "    logprobs = np.multiply(one_hot, np.log(f_pass['a_output'])) + np.multiply((1 - one_hot), np.log(1 - f_pass['a_output']))\n",
    "    L = (-1.0/m) * np.sum(logprobs)\n",
    "    \n",
    "    # L2 regularize\n",
    "    sum_w = np.sum(np.square(W_hidden)) + np.sum(np.square(W_output))\n",
    "    L = L + (sum_w * (L2_lambda/(2*m)))\n",
    "    L = np.squeeze(L)\n",
    "    \n",
    "    # Chain Rule\n",
    "#     dZ2 = f_pass['a_output'] - one_hot \n",
    "#     dW2 = dZ2 * sigmoid_gradient(f_pass['a_output'])\n",
    "    \n",
    "#     dz1= dW2.dot(W_output.T)\n",
    "#     dW1= dz1* tanh_gradient(f_pass['a_hidden']) \n",
    "    \n",
    "#     W_hidden_grad = np.concatenate([np.ones((W_hidden_grad.shape[0], 1)), W_hidden_grad], axis=1)\n",
    "#     W_hidden_grad += (X_input[:,:].T.dot(dW1))\n",
    "#     W_output_grad += f_pass['a_hidden'].T.dot(dW2)\n",
    "    \n",
    "    # Chain-Rule\n",
    "#     W_hidden_grad = W_hidden - tanh_gradient(f_pass['a_hidden']) + ((L2_lambda/m) * W_hidden)\n",
    "#     W_output_grad = W_output - sigmoid_gradient(f_pass['a_output']) + ((L2_lambda/m) * W_output)\n",
    "\n",
    "#     # 3rd variant\n",
    "#     W_hidden_grad =  W_hidden_grad - L2_lambda * np.dot(X_input.T, np.dot(a_hidden[:,1:],(np.dot((a_output-one_hot).T ,a_hidden[:,1:])).T))\n",
    "#     W_output_grad = W_output_grad - L2_lambda * np.dot(a_hidden.T, a_output-one_hot)\n",
    "    \n",
    "    # 4th variant\n",
    "    # Gradient at output layer\n",
    "    dZ2 = sigmoid_gradient(a_output)\n",
    "    dW2 = np.dot(a_hidden[:,1:].T,dZ2)/m + ((L2_lambda / m) * W_output[1:,:])\n",
    "    dB2 = np.sum(dZ2, axis=0)/m\n",
    "    \n",
    "    #gradient at hidden layer\n",
    "    dZ1 = np.multiply(dZ2.dot(W_output[1:,].T), tanh_gradient(a_hidden[:,1:]))\n",
    "    dW1 = (np.dot(X.T, dZ1))/m + ((L2_lambda / m) * W_hidden[1:,:])\n",
    "    dB1 = np.sum(dZ1, axis=0)/m\n",
    "    \n",
    "    dB1 = dB1.reshape(1,10)\n",
    "    dB2 = dB2.reshape(1,3)\n",
    "    \n",
    "    #a_hidden = np.concatenate([np.ones(( X.shape[0], 1)), a_hidden], axis=1)\n",
    "    W_hidden_grad = np.concatenate((dB1, dW1), axis = 0)\n",
    "    W_output_grad = np.concatenate((dB2, dW2), axis = 0)\n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
    "\n",
    "    return L, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param initial_W: initial weights as a 1D array.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_epoch: number of iterations\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "\n",
    "    options = {'maxiter': num_epoch}\n",
    "\n",
    "    ###########################################################################################\n",
    "    # Full Mark: 1                                                                            #\n",
    "    # TODO:                                                                                   #\n",
    "    # Optimize weights                                                                        #\n",
    "    # (Hint: use scipy.optimize.minimize to automatically do the iteration.)                  #\n",
    "    # ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) #\n",
    "    # For some optimizers, you need to set 'jac' as True.                                     #\n",
    "    # You may need to use lambda to create a function with one parameter to wrap the          #\n",
    "    # loss_funtion you have implemented above.                                                #\n",
    "    #                                                                                         #\n",
    "    # Note that scipy.optimize.minimize only accepts a 1D weight array as initial weights,    #\n",
    "    # and the output optimized weights will also be a 1D array.                               #\n",
    "    # That is why we unroll the initial weights into a single long vector.                    #\n",
    "    ###########################################################################################\n",
    "    \n",
    "    W_final = np.copy(initial_W)\n",
    "    \n",
    "    def fun(W):\n",
    "        global W_final\n",
    "        L, grad = loss_funtion(W, X, y, num_feature, num_hidden, num_output, L2_lambda)\n",
    "        W_final = W - grad\n",
    "        return L\n",
    "    \n",
    "    minimize(fun, initial_W, options=options)\n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # Obtain W_hidden and W_output back from W_final\n",
    "    W_hidden = np.reshape(W_final[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "    W_output = np.reshape(W_final[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    return [W_hidden, W_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, y_test, W):\n",
    "    '''\n",
    "    :param X_test: input\n",
    "    :param y_test: labels of X_test\n",
    "    :param W: a list containing two weights W_hidden and W_output.\n",
    "    '''\n",
    "\n",
    "    test_input = np.concatenate([np.ones((y_test.size, 1)), X_test], axis=1)\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Predict on test set and compute the accuracy.                                   #\n",
    "    # (Hint: use forward function to get predicted output)                            #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    cache = forward(W, test_input)\n",
    "    \n",
    "    # One hot encoding\n",
    "    shape = (y_test.size, y_test.max()+1)\n",
    "    one_hot = np.zeros(shape)\n",
    "    rows = np.arange(y_test.size)\n",
    "    one_hot[rows, y_test] = 1\n",
    "    \n",
    "    # Encode our outputs\n",
    "    y_hat = np.zeros(shape)\n",
    "    for i, r in enumerate(cache['a_output']):\n",
    "        max_val = np.amax(r)\n",
    "        for j, v in enumerate(r):\n",
    "            if max_val == v:\n",
    "                y_hat[i,j] = 1\n",
    "    \n",
    "    # Find Accuracy\n",
    "    acc = np.mean(y_hat == one_hot)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this part #\n",
    "# Define parameters\n",
    "NUM_FEATURE = 4\n",
    "NUM_HIDDEN_UNIT = 10\n",
    "NUM_OUTPUT_UNIT = 3\n",
    "NUM_EPOCH = 100\n",
    "L2_lambda = 1\n",
    "\n",
    "# Load data\n",
    "X, y = load_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Initialize weights\n",
    "initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
    "initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
    "initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
    "\n",
    "# Neural network learning\n",
    "W = optimize(initial_W, X_train, y_train, NUM_EPOCH, NUM_FEATURE, NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT, L2_lambda)\n",
    "\n",
    "# Predict\n",
    "acc = predict(X_test, y_test, W)\n",
    "print(\"Test accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
